{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTvIwDlYvBzC"
      },
      "source": [
        "# HW1: LeNet-5 with Post-training Quantization and Quantization Aware Training\n",
        "[LeNet](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) is considered to be the first ConvNet.\n",
        "We are going to implement a neural architecture similar to LeNet and train it with [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
        "\n",
        "Before we start, you may check this [Tensorspace-LeNet](https://tensorspace.org/html/playground/lenet.html) to play with LeNet and get familiar with this neural architecture.\n",
        "\n",
        "![image](https://production-media.paperswithcode.com/methods/LeNet_Original_Image_48T74Lc.jpg)\n",
        "Ref.: LeCun et al., Gradient-Based Learning Applied to Document Recognition, 1998a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMbQ1FFgQs0K"
      },
      "source": [
        "## 1. Initial Setup\n",
        "\n",
        "We are going to implement and train this nerual network with PyTorch.\n",
        "If you are not familer with PyTorch, check [official tutorail](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
        "\n",
        "**Reminder:** set the runtime type to \"GPU\", or your code will run much more slowly on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbiiMcdNJI--",
        "outputId": "7b1465a7-bda6-4988-cf51-5ad7972e0535"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import random\n",
        "from quantutils import copy_model\n",
        "from copy import deepcopy\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCaMDWYArEXO"
      },
      "source": [
        "### 1.1 Load dataset\n",
        "Load training and test data from the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5UuOjjrnogR",
        "outputId": "0508f668-658c-404b-d18d-ae271305a19a"
      },
      "outputs": [],
      "source": [
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize((32, 32)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=0,\n",
        "                                          worker_init_fn=seed_worker, generator=g,)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=8,\n",
        "                                         shuffle=False, num_workers=0,\n",
        "                                         worker_init_fn=seed_worker, generator=g,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l62CkyIwtSOv"
      },
      "source": [
        "### 1.2 Define the Neural Network\n",
        "Define a simple CNN that classifies MNIST images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fL3F-7Rntog"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(1, 6, 5, bias=False)),\n",
        "            ('relu', nn.ReLU()),\n",
        "        ]))\n",
        "\n",
        "        self.maxpool2 = nn.Sequential(OrderedDict([\n",
        "            ('pool', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "\n",
        "        self.conv3 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(6, 16, 5, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "\n",
        "        self.maxpool4 = nn.Sequential(OrderedDict([\n",
        "            ('pool', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "\n",
        "        self.conv5 = nn.Sequential(OrderedDict([\n",
        "            ('conv', nn.Conv2d(16, 120, 5, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "\n",
        "        self.fc6 = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(120, 84, bias=False)),\n",
        "            ('relu', nn.ReLU())\n",
        "        ]))\n",
        "\n",
        "        self.output = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(84, 10, bias=False)),\n",
        "        ]))\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc6(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "NET = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyOAi8OFQs0N"
      },
      "source": [
        "### 1.3 Question: Profile the Neural Architecture by TorchInfo\n",
        "Torchinfo provides information complementary to what is provided by print(your_model) in PyTorch, similar to Tensorflow's model.summary() API to view the visualization of the model, which is helpful while debugging your network. Check this [link](https://github.com/TylerYep/torchinfo#how-to-use) about how to use TorchInfo by `summary()` and fill in the TODO in the following cell. You should get the result similar to the table below:\n",
        "\n",
        "```\n",
        "==========================================================================================\n",
        "Layer (type:depth-idx)                   Output Shape              Param #\n",
        "==========================================================================================\n",
        "LeNet                                    --                        --\n",
        "...\n",
        "...\n",
        "==========================================================================================\n",
        "Total params: ...\n",
        "...\n",
        "Estimated Total Size (MB): ...\n",
        "==========================================================================================\n",
        "```\n",
        "\n",
        "\n",
        "Ref.: https://github.com/TylerYep/torchinfo\n",
        "\n",
        "Please read *B. LeNet-5* in the [original paper](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) and answer the following questions.\n",
        "1. Include the output of `summary()` into the report \n",
        "2. Provide the type (convolution, pooling, fully-connected layer, etc.), input activation size, output activation size, and activation function (if any) of each layer in a table format.\n",
        "3. What is the difference between this neural architecture and the lenet-5 in the original paper?\n",
        "4. Could we replace the 3rd conv, the conv in conv5, with a fully connected layer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n50e9PASQs0N",
        "outputId": "967cd542-e12c-41a0-b578-2de0415850cd"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nijieuxptag6"
      },
      "source": [
        "### 1.4 Train and Test the Neural Network\n",
        "Train this CNN on the training dataset (this may take a few moments).\n",
        "* Check how to save and load the model\n",
        "    * https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "    * Save:\n",
        "        ```\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        ```\n",
        "    * Load:\n",
        "        ```\n",
        "        model = TheModelClass(*args, **kwargs)\n",
        "        model.load_state_dict(torch.load(PATH))\n",
        "        model.eval()\n",
        "        ```\n",
        "* After training the model, we will save it as `lenet.pt`.\n",
        "* You can comment out `train(NET, trainloader, 2)` and uncomment `NET.load_state_dict(torch.load('lenet.pt'))` to load the trained model. \n",
        "    * Reloading the model from `lenet.pt` can save your time if there is something wrong and you need to restart and run all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzK6ohj5oNCT"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module, dataloader: DataLoader, num_epoch):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(dataloader):\n",
        "\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 2000))\n",
        "                running_loss = 0.0\n",
        "        print(test(model, testloader, None))\n",
        "    print('Finished Training')\n",
        "\n",
        "def test(model: nn.Module, dataloader: DataLoader, max_samples=None, device=device) -> float:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    n_inferences = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if max_samples:\n",
        "                n_inferences += inputs.shape[0]\n",
        "                if n_inferences > max_samples:\n",
        "                    break\n",
        "\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIeiyuPvQs0O",
        "outputId": "845a588a-4946-4319-a613-f04573ed4653"
      },
      "outputs": [],
      "source": [
        "train(NET, trainloader, 2)\n",
        "# NET.load_state_dict(torch.load('lenet.pt'))\n",
        "score = test(NET, testloader, None)\n",
        "print('Accuracy of the network on the test images: {}%'.format(score))\n",
        "\n",
        "torch.save(NET.state_dict(), 'lenet.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQZoEjBSveV8"
      },
      "source": [
        "## 2. Post-training Quantization\n",
        "### 2.1 Question: Visualize Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qKRX7ply7I2"
      },
      "source": [
        "We have flattened all vector for you by `tensor.view(-1)`.\n",
        "\n",
        "1. Try plotting a histogram of each weight, put results in the report. hint: `np.histogram()` and `plt.hist()`\n",
        "\n",
        "1. Record the range of the weights, as well as their 3-sigma range (the difference between $\\mu + 3\\sigma$ and $\\mu - 3\\sigma$).\n",
        "For which layers is the 3-sigma range larger or smaller than the actual range?\n",
        "2. Explain which range you would prefer to use if you were to quantize each layer's weights and wanted to strike a balance between the range of values that could be expressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "N2h7zJ8m3GAF",
        "outputId": "1f73df65-73a1-413e-cfec-ebca385da80a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conv1_weights = NET.conv1[0].weight.data.cpu().view(-1)\n",
        "conv2_weights = NET.conv3[0].weight.data.cpu().view(-1)\n",
        "conv3_weights = NET.conv5[0].weight.data.cpu().view(-1)\n",
        "fc1_weights = NET.fc6[0].weight.data.cpu().view(-1)\n",
        "fc2_weights = NET.output[0].weight.data.cpu().view(-1)\n",
        "\n",
        "weightDict = {\n",
        "    'conv1_weights':conv1_weights,\n",
        "    'conv2_weights': conv2_weights,\n",
        "    'conv3_weights': conv3_weights,\n",
        "    'fc1_weights': fc1_weights,\n",
        "    'fc2_weights':fc2_weights\n",
        "}\n",
        "\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hKjshaHD11m"
      },
      "source": [
        "### 2.2 Question:  Quantize Weights\n",
        "Computation of convolution or fully-connected layer can be expressed as\n",
        "\n",
        "$$W \\times I = O$$\n",
        "\n",
        "where $W$ is the weight tensor, $I$ is the input tensor, and $O$ is the output tensor.\n",
        "Let $s_W$ be the scaling factor. We have\n",
        "\n",
        "$$s_W W_q \\times I \\approx W \\times I = O$$\n",
        "\n",
        "where $W_q$ is the quantized 8-bit signed integer weight tensor.\n",
        "\n",
        "Fill in the TODO in `quantized_weights()` located in `quantutils.py`. If you’ve done everything correctly, the accuracy degradation should be negligible (~1%).\n",
        "1. What is $s_W$? Explain how you get it.\n",
        "2. What is the accuracy degradation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orOwTnXxU1nb"
      },
      "outputs": [],
      "source": [
        "from quantutils import quantized_weights\n",
        "net_q2 = copy_model(NET)\n",
        "\n",
        "def quantize_layer_weights(model: nn.Module):\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "            q_layer_data, scale = quantized_weights(layer.weight.data)\n",
        "            q_layer_data = q_layer_data.to(device)\n",
        "\n",
        "            layer.weight.data = q_layer_data\n",
        "            layer.weight.scale = scale\n",
        "\n",
        "            if (q_layer_data < -128).any() or (q_layer_data > 127).any():\n",
        "                raise Exception(\"Quantized weights of {} layer include values out of bounds for an 8-bit signed integer\".format(layer.__class__.__name__))\n",
        "            if (q_layer_data != q_layer_data.round()).any():\n",
        "                raise Exception(\"Quantized weights of {} layer include non-integer values\".format(layer.__class__.__name__))\n",
        "\n",
        "quantize_layer_weights(net_q2)\n",
        "score = test(net_q2, testloader)\n",
        "print('Accuracy of the network after quantizing all weights: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg7bfTF1bBVe"
      },
      "source": [
        "### 2.3 Question: Visualize Activations\n",
        "1. Try plotting histograms the input images and the output activations of each operation, put results in the report. hint: `np.histogram()` and `plt.hist()`\n",
        "\n",
        "2. Record the range of the values, as well as their 3-sigma range (the difference between $\\mu + 3\\sigma$ and $\\mu - 3\\sigma$).\n",
        "For which layers is the 3-sigma range larger or smaller than the actual range?\n",
        "3. Explain which range you would prefer to use if you were to quantize each layer's output activations and wanted to strike a balance between the range of values that could be expressed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcv-7s5WQs0P",
        "outputId": "a53c58b7-86ca-4fe6-b796-b1d8bf04093a"
      },
      "outputs": [],
      "source": [
        "net_q3 = copy_model(NET)\n",
        "\n",
        "def visualize_activations(module, input, output):\n",
        "    if module.profile_activations is True:\n",
        "        module.inAct = input[0].cpu().reshape(-1)\n",
        "        module.outAct = output[0].cpu().reshape(-1)\n",
        "\n",
        "for name, model in net_q3.named_children():\n",
        "    print(\"{}\\n [register_forward_hook]: {}\".format(name, model))\n",
        "    model.profile_activations = True\n",
        "    model.register_forward_hook(visualize_activations)\n",
        "net_q3.eval()\n",
        "with torch.no_grad():\n",
        "    input = trainset[0][0].unsqueeze(0)\n",
        "    _ = net_q3(input.to(device))\n",
        "for name, model in net_q3.named_children():\n",
        "    model.profile_activations = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "AEo8VK46bwjn",
        "outputId": "49000a0f-6acd-4cf7-f070-38a497c50da1"
      },
      "outputs": [],
      "source": [
        "input_activations = net_q3.conv1.inAct\n",
        "conv1_output_activations = net_q3.conv1.outAct\n",
        "conv3_output_activations = net_q3.conv3.outAct\n",
        "conv5_output_activations = net_q3.conv5.outAct\n",
        "fc6_output_activations = net_q3.fc6.outAct\n",
        "output_output_activations = net_q3.output.outAct\n",
        "\n",
        "actDict = {\n",
        "    'input_activations':input_activations,\n",
        "    'conv1_output_activations':conv1_output_activations,\n",
        "    'conv3_output_activations':conv3_output_activations,\n",
        "    'conv5_output_activations':conv5_output_activations,\n",
        "    'fc6_output_activations':fc6_output_activations,\n",
        "    'output_output_activations':output_output_activations\n",
        "}\n",
        "\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haiPVx4ibEra"
      },
      "source": [
        "### 2.4 Question:  Quantize Activations\n",
        "The output of conv in conv1 can be expressed as \n",
        "$$W_{conv1}\\times I=O_{conv1}.$$\n",
        "\n",
        "Let the scaling factor of the input matrix $I$ be $s_I$, the scaling factor of the weight matrix $W_{conv1}$\n",
        "be $s_{W_{conv1}}$, and the scaling factor of the output matrix $O_{conv1}$ be $s_{O_{conv1}}$. Then we have\n",
        "\n",
        "$$s_{W_{conv1}} W_{conv1_q} \\times s_I I_q \\approx W_{conv1} \\times I = O_{conv1} \\approx s_{O_{conv1}}O_{conv1_q}$$\n",
        "\n",
        "where $W_{conv1_q}$ is the quantized 8-bit signed integer weight tensor, $I_q$ is the quantized 8-bit signed integer input activation tensor, and $O_{conv1_q}$ is the quantized 8-bit signed integer output activation tensor.\n",
        "\n",
        "Since we're doing post-training quantization, we can get $s_I$, $s_{W_{conv1}}$, and $s_{O_{conv1}}$ first and do the calculations of a scaling factor $M_1$. The layer output should be multiplied by $M_{layer}$ before being fed into the next layer.\n",
        "\n",
        "As for `forward()` of `NetQuantized()` in `quantutils.py`, make sure you can simulate fixed-point representation when doing any calculation with input/output scale. Keep in mind that we will implement hardware to accelerate this model with fixed-point computations.\n",
        "* In this assignment, we only \"emulate\" fixed-point computations. We don't need to use any fixed-point data type (e.g., `int`).\n",
        "* You will have to fill in the TODO in `forward()` to scale the outputs of each layer. Consider rounding binary fractions to the 16th place with the following steps (e.g., for output_scale):\n",
        "  1. `scale = round(scale*(2**16))`: Now, we have the `scale` rounded to the 16th place with a software trick of moving the binary point (`*(2**16)`) and applying the round function.\n",
        "  2. `(scale*features) >> 16`: Move back the binary point.\n",
        "  3. Two options for emulating bit-shifting in fixed-point numbers instead of floating-point numbers:\n",
        "       - 1.`(scale*features) >> 16`. Make sure that the data type of `(scale*features)` is **int**.\n",
        "       - 2.`floor((scale*features) >> 16)`\n",
        "  4. Clamp the value between -128 and 127\n",
        "* Also, don't forget to scale the input features before feeding them into the first layer.\n",
        "  * Originally, we may perform the computation with `round(input_features/s_I)`. However, since `1/s_I` is much greater than 1, we prefer to use `round(1/s_I)*input_features` for hardware implementation considerations. This is the reason why we ask you to return `1/s_I` in `quantize_initial_input()`.\n",
        "  * The `input_scale` stored in the model should be `1/s_I`.\n",
        "    \n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "1. How to compute $s_I$, $s_{W_{conv1}}$, and $s_{O_{conv1}}$?\n",
        "2. The true quantized output activation tensor depends on $W_{conv1_q}$ and $I_q$, so we cannot simply apply only $s_{O_{conv1}}$ on the output of $W_{conv1_q}\\times I_q$ to re-quantize the output activation. \\\n",
        "Derive an equation for the quantized output of the conv in conv1 after quantizing input activation and weight with $s_I$ and $s_{W_{conv1}}$. And show the scaling factor $M_1$. \\\n",
        "(hint: $M_1$ is the scaling factor of the $O'$ in $W_{conv1_q} \\times I_q = O'$, such that $M_1\\times(W_{conv1_q} \\times I_q) = M_1\\times(O') = O_q$)\n",
        "3. Derive an equation for the quantized output of the conv in conv3 after quantizing input activation and weight.\n",
        "4. Show the general equantion of each layer for calculating the scaling factor $M$ of output activation.\n",
        "\n",
        "Fill in the TODO in `class NetQuantized(nn.Module)` located in `quantutils.py`. If you’ve done everything correctly, the accuracy degradation should be negligible (~1%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "13CpHgvE994J",
        "outputId": "fb7e551d-0866-4501-b7c8-9027d07ea069"
      },
      "outputs": [],
      "source": [
        "from quantutils import NetQuantized\n",
        "\n",
        "net_init = copy_model(net_q2)\n",
        "net_init.input_activations = deepcopy(net_q3.conv1.inAct)\n",
        "\n",
        "for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n",
        "    layer_init.inAct = deepcopy(layer_q3.inAct)\n",
        "    layer_init.outAct = deepcopy(layer_q3.outAct)\n",
        "\n",
        "net_quantized = NetQuantized(net_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcBXEodN6hrY",
        "outputId": "b6500eaa-d0da-48bb-9a6e-36ef6e995c53"
      },
      "outputs": [],
      "source": [
        "score = test(net_quantized, testloader)\n",
        "print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-y3ioyHQs0R"
      },
      "source": [
        "Answer the following questions.(hint: please consider verilog implementation):\n",
        "\n",
        "6. What is the benefit of using `floor`?\n",
        "7. What is the benefit of replacing `x*output_scale` with `x/round(1/output_scale)`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFYJQKXTQs0R",
        "outputId": "0808fe8f-ed26-4b59-9813-6dcc709b93ca"
      },
      "outputs": [],
      "source": [
        "print(\"input_scale:\\n\", net_quantized.input_scale.item())\n",
        "print(\"output_scale:\\n {}\\n {}\\n {}\\n {}\\n {}\".format(\n",
        "    net_quantized.conv1.output_scale.item(),\n",
        "    net_quantized.conv3.output_scale.item(),\n",
        "    net_quantized.conv5.output_scale.item(),\n",
        "    net_quantized.fc6.output_scale.item(),\n",
        "    net_quantized.output.output_scale.item()\n",
        "))\n",
        "\n",
        "print(\"input_scale:\\n\", net_quantized.input_scale.item())\n",
        "print(\"output_scale:\\n {}\\n {}\\n {}\\n {}\\n {}\".format(\n",
        "    round(1/net_quantized.conv1.output_scale.item()),\n",
        "    round(1/net_quantized.conv3.output_scale.item()),\n",
        "    round(1/net_quantized.conv5.output_scale.item()),\n",
        "    round(1/net_quantized.fc6.output_scale.item()),\n",
        "    round(1/net_quantized.output.output_scale.item())\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jTOL7scbMs7"
      },
      "source": [
        "### 2.5 Question:  Quantize Biases\n",
        "We add a bias in the final layer of this LeNet. Now the equation is\n",
        "\n",
        "$$s_W W_q \\times s_I I_q + s_{\\beta} \\beta_q \\approx W \\times I + \\beta = O \\approx s_O O_q$$\n",
        "\n",
        "where $s_W$ is the scaling factor of the weight matrix $W$, $s_I$ is the scaling factor of the input matrix $I$, $s_{\\beta}$ is the scaling factor of the bias $\\beta$ and, $s_O$ is the scaling factor of the output matrix $O$.\n",
        "\n",
        "Note that our biases are commonly quantized to 32-bits\n",
        "\n",
        "* You can comment out `train(NET_WITH_BIAS, trainloader, 2)` and uncomment `NET_WITH_BIAS.load_state_dict(torch.load('lenet_with_bias.pt'))` to load the quantized model.\n",
        "    * Reloading the model from `lenet_with_bias.pt` can save your time if there is something wrong and you need to restart and run all.\n",
        "\n",
        "Answer the following questions.\n",
        "1. What is the scaling factor for the bias?\\\n",
        "(hint: we want the equation to be $M \\times(W_q \\times I_q + \\beta_q) = O_q$)\n",
        "\n",
        "Fill in the TODO in `class NetQuantizedWithBias(NetQuantized)` located in `quantutils.py`. If you’ve done everything correctly, the accuracy degradation should be negligible (~1%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvv9-k1HPbgz"
      },
      "outputs": [],
      "source": [
        "class NetWithBias(Net):\n",
        "    def __init__(self):\n",
        "        super(NetWithBias, self).__init__()\n",
        "\n",
        "        self.output = nn.Sequential(OrderedDict([\n",
        "            ('fc', nn.Linear(84, 10, bias=True)),\n",
        "        ]))\n",
        "\n",
        "NET_WITH_BIAS = NetWithBias().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vLUCDnnVf4R",
        "outputId": "fc17ea83-a15f-470f-c6c8-ed92ecdea218"
      },
      "outputs": [],
      "source": [
        "train(NET_WITH_BIAS, trainloader, 2)\n",
        "# NET_WITH_BIAS.load_state_dict(torch.load('lenet_with_bias.pt'))\n",
        "score = test(NET_WITH_BIAS, testloader)\n",
        "print('Accuracy of the network (with a bias) on the test images: {}%'.format(score))\n",
        "torch.save(NET_WITH_BIAS.state_dict(), 'lenet_with_bias.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ZiJk6yEEM-",
        "outputId": "1c4a1ba4-d11b-4e11-df0c-a00508c5f180",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for name, model in NET_WITH_BIAS.named_children():\n",
        "    # print(\"{}\\n [register_forward_hook]: {}\".format(name, model))\n",
        "    model.profile_activations = True\n",
        "    model.register_forward_hook(visualize_activations)\n",
        "NET_WITH_BIAS.eval()\n",
        "with torch.no_grad():\n",
        "    input = trainset[0][0].unsqueeze(0)\n",
        "    _ = NET_WITH_BIAS(input.to(device))\n",
        "for name, model in NET_WITH_BIAS.named_children(): model.profile_activations = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZwk8KLtAUAM",
        "outputId": "39e35485-ae01-4457-ab9a-96f98b1feb50"
      },
      "outputs": [],
      "source": [
        "net_with_bias_with_quantized_weights = copy_model(NET_WITH_BIAS)\n",
        "quantize_layer_weights(net_with_bias_with_quantized_weights)\n",
        "\n",
        "score = test(net_with_bias_with_quantized_weights, testloader)\n",
        "print('Accuracy of the network on the test images after all the weights are quantized but the bias isn\\'t: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJvR6Wv_GJJX",
        "outputId": "6c572739-75f4-4931-a0f4-d93a9c16d1d9"
      },
      "outputs": [],
      "source": [
        "from quantutils import NetQuantizedWithBias\n",
        "net_quantized_with_bias = NetQuantizedWithBias(net_with_bias_with_quantized_weights)\n",
        "\n",
        "score = test(net_quantized_with_bias, testloader)\n",
        "print('Accuracy of the network on the test images after all the weights and the bias are quantized: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFKPOnHG4_y2"
      },
      "source": [
        "## 3. Quantization Aware Training (QAT)\n",
        "Before proceeding with Part 3, please consider the following points:\n",
        "1. To convert QAT models to models with zero point as zero for easier hardware implementation in the future, we have switched the backend from fbgemm to qnnpack. However, not all devices support qnnpack. \n",
        "* Try `print(torch.backends.quantized.supported_engines)` to check if qnnpack is supported. If qnnpack is not listed, try switch to using Colab. \n",
        "* When using Colab, please select CPU and avoid selecting GPU or other accelerators to ensure qnnpack can run properly.\n",
        "* <font color='red'>After switching the device to CPU, make sure to rerun 1.4, cell of `def train`, to ensure device consistency. </font>\n",
        "2. You can comment out `train(MODEL_FP32, trainloader, 2)` and uncomment `MODEL_FP32.load_state_dict(torch.load('MODEL_FP32.pt'))` to load the model.\n",
        "    * Reloading the model from `MODEL_FP32.pt` can save your time if there is something wrong and you need to restart and run all.\n",
        "\n",
        "### 3.1 Question: QAT\n",
        "Try to trace code and study the quantization-aware training (QAT) from [Quantization — PyTorch 1.13 documentation](https://pytorch.org/docs/stable/quantization.html), then answer the following question.\n",
        "\n",
        "1. How can the QAT achieve a higher accuracy than the post-training quantization (PTQ)?\n",
        "2. Two more layers (quant, dequant) appeared after we quantized our model using the PyTorch QAT method. What do these two layers do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruot6qC6pPcG"
      },
      "outputs": [],
      "source": [
        "#check if qnnpack is supported\n",
        "print(torch.backends.quantized.supported_engines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#switch device to cpu if needed\n",
        "device=torch.device('cpu')\n",
        "#rerun 1.4, cell of `def train` now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TfUxMTyQs0S"
      },
      "outputs": [],
      "source": [
        "class QATNet(NetWithBias):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = super().forward(x)\n",
        "        x = self.dequant(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xW7vPyyjCA-"
      },
      "outputs": [],
      "source": [
        "qcfg = torch.quantization.get_default_qat_qconfig('qnnpack')\n",
        "qact = torch.quantization.FakeQuantize.with_args(observer=torch.quantization.MovingAverageMinMaxObserver,\n",
        "                             quant_min=-128, quant_max=127, dtype=torch.qint8,\n",
        "                             qscheme=torch.per_tensor_symmetric, reduce_range=False)\n",
        "qcfg = torch.quantization.QConfig(activation=qact, weight=qcfg.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcbodizcjCA-",
        "outputId": "3bdfc629-d821-4d3d-fb16-91166758df86"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "MODEL_FP32 = QATNet().to(device)\n",
        "\n",
        "# prepare the model for QAT\n",
        "MODEL_FP32.train()\n",
        "# Specify quantization configuration\n",
        "torch.backends.quantized.engine = 'qnnpack'\n",
        "MODEL_FP32.qconfig = qcfg\n",
        "torch.quantization.prepare_qat(MODEL_FP32, inplace=True)\n",
        "\n",
        "train(MODEL_FP32, trainloader, 2)\n",
        "# MODEL_FP32.load_state_dict(torch.load('MODEL_FP32.pt'))\n",
        "score = test(MODEL_FP32, testloader)\n",
        "print('Accuracy of the MODEL_FP32: {}%'.format(score))\n",
        "torch.save(MODEL_FP32.state_dict(), 'MODEL_FP32.pt')\n",
        "\n",
        "# convert the model to a quantized model\n",
        "torch.quantization.convert(MODEL_FP32, inplace=True)\n",
        "\n",
        "# evaluate the model on the test set\n",
        "MODEL_FP32.eval()\n",
        "\n",
        "\n",
        "device=torch.device('cpu')\n",
        "score = test(MODEL_FP32, testloader)\n",
        "print('Accuracy of the quantized LeNet-5 model on the test images: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_W-RCmHjlkg"
      },
      "source": [
        "### Extract weight, floating-point bias and floating-point scale of activations and weights of every layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOaCXxI5jCA_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "# It is easier to download all the files with zip\n",
        "zf = zipfile.ZipFile('parameters.zip', 'w', zipfile.ZIP_DEFLATED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9JwkJMNjCA_"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./weights'):\n",
        "    os.mkdir('./weights')\n",
        "if not os.path.exists('./float_scale'):\n",
        "    os.mkdir('./float_scale')\n",
        "\n",
        "for name, weights in MODEL_FP32.state_dict().items():\n",
        "    name_split = name.split('.')\n",
        "    if(weights!= None):\n",
        "      if(name_split[-2] != \"_packed_params\"):\n",
        "        if(weights.type()== \"torch.quantized.QInt8Tensor\" or weights.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "          np.savetxt('./weights/%s.csv' %(name) , weights.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "          zf.write('./weights/%s.csv' %(name))\n",
        "\n",
        "          np.savetxt('./float_scale/%s.scale.csv' %(name) , np.array([weights.q_scale()]), delimiter=',')\n",
        "\n",
        "          np.savetxt('./float_scale/%s.zero_point.csv' %(name) , np.array([weights.q_scale()]), delimiter=',')\n",
        "        else:\n",
        "          np.savetxt('./float_scale/%s.csv' %(name) , weights.cpu().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "\n",
        "      elif(name_split[-1] == \"_packed_params\"):\n",
        "        if not os.path.exists('./weights/_packed_params'):\n",
        "          os.mkdir('./weights/_packed_params')\n",
        "        name = name_split[0]+\".\"+name_split[1]\n",
        "        weight, bias = weights\n",
        "        if(weight.type()== \"torch.quantized.QInt8Tensor\" or weight.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "          np.savetxt('./weights/%s.weight.csv' %(name)  , weight.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "          zf.write('./weights/%s.weight.csv' %(name))\n",
        "\n",
        "          np.savetxt('./float_scale/%s.weight.scale.csv' %(name) , np.array([weight.q_scale()]), delimiter=',')\n",
        "\n",
        "          np.savetxt('./float_scale/%s.weight.zero_point.csv' %(name) , np.array([weight.q_zero_point()]), delimiter=',')\n",
        "\n",
        "        if(bias != None):\n",
        "\n",
        "          if(bias.type()== \"torch.quantized.QInt8Tensor\" or bias.type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "            np.savetxt('./float_scale/%s.bias.csv' %(name) , bias.cpu().int_repr().numpy().reshape(-1).astype(float), delimiter=',')\n",
        "          else:\n",
        "            np.savetxt('./float_scale/%s.bias.csv' %(name) , bias.cpu().detach().numpy().reshape(-1).astype(float), delimiter=',')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeMc2DAFjCBA"
      },
      "source": [
        "### 3.2 Caculate the fixed-point output scale of the QAT model\n",
        "In function `float_to_fixed_scale()` located in `quantutils.py`, first you need to calculate fixed-point scales $M_{l}$. Store them in the `scalesDict` dictionary. Second,  calculate a revised value for the `outputBias` variable.\n",
        "\n",
        "\n",
        "In the front, we extract the scales of input, output, and weight  for each layer from the model. By [pytorch Quantized Tensor](https://pytorch.org/docs/stable/quantization.html), the scaling factor of the input matrix $I_{l}$,  weight matrix $W_{l}$ and output matrix $O_{l}$ are denoted by $s_{W_{l}}$, $s_{I_{l}}$ and $s_{O_{l}}$ , respectively, for each layer ${l}$. The corresponding quantized 8-bit signed integer tensors are denoted by $I_{l_q}$, $W_{l_q}$ and $O_{l_q}$.\n",
        "$$I_l = I_{l_q} * s_{I_{l}},$$\n",
        "$$W_l = W_{l_q} * s_{W_{l}},$$\n",
        "$$O_l = O_{l_q} * s_{O_{l}}.$$\n",
        "\n",
        "In `float_to_fixed_scale()` TODO located in `quantutils.py`, we need to caculate $M_{l}$ and save them in `scalesDict` where in layer $l$,\n",
        "$$W_{l_q}* I_{l_q} * M_{l} \\approx O_{l_q} = I_{{l+1}_q}$$\n",
        "As for the initial input, we need to caculate $M_{quant}$ where\n",
        "$$I_{initialInput} * M_{quant} \\approx O_{{initialInput}_q} = I_{{conv1.conv}_q}$$\n",
        "*   hint: `act_scalesDict[layerName]` store the output activation scale, i.e., $s_{O_{l}}$ in the above equation, and it is also the input activation scale of the next layer $s_{I_{l+1}}$.\n",
        "\n",
        "\n",
        "\n",
        "To simplify the hardware implementation, let's convert $M_{l}$ into integer by multiplying `2**16` and rounding it. As for the initial input, you can simply round it since $M_{quant}$ is much greater than 1.\n",
        "\n",
        "In addition, you need to adjust the `outputBias` value to ensure that it remains unchanged when we use $M_{output}$ for requantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "act_scalesDict = {}\n",
        "weight_scalesDict = {}\n",
        "outputBias_float = []\n",
        "\n",
        "layerName = [\"conv1.conv\", \"conv3.conv\", \"conv5.conv\", \"fc6.fc\", \"output.fc\"]\n",
        "for key in layerName:\n",
        "    Arr = np.loadtxt('./float_scale/'+key+\".weight.scale.csv\",\n",
        "                        delimiter=',').reshape(([1])).astype(float)\n",
        "    weight_scalesDict[key] = Arr\n",
        "\n",
        "    Arr = np.loadtxt('./float_scale/'+key+\".scale.csv\",\n",
        "                        delimiter=',').reshape(([1])).astype(float)\n",
        "    act_scalesDict[key] = Arr\n",
        "\n",
        "act_scalesDict[\"quant\"] = np.loadtxt(\"./float_scale/quant.scale.csv\",\n",
        "                    delimiter=',').reshape(([1])).astype(float)\n",
        "outputBias_float = np.loadtxt(\n",
        "    './float_scale/'+key+\".bias.csv\", delimiter=',').reshape(([1, 10])).astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify the correctness of the implementation for above TODO, try print the result of float_to_fixed_scale().\n",
        "The range of the result should be similar to the following:\n",
        "```\n",
        "({'conv1.conv': 93, 'conv3.conv': 78, 'conv5.conv': 243, 'fc6.fc': 284, 'output.fc': 230, 'quant': 127}, array([[  29,  -52, -159,   38, -160,  -17,   -3,  -35,  145, -166]]))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awe6IPlYjCBA",
        "outputId": "2ebaf1f3-56b6-4b7a-9164-6b45968d8d37"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from quantutils import float_to_fixed_scale\n",
        "\n",
        "scalesDict, outputBias = float_to_fixed_scale(act_scalesDict, weight_scalesDict, outputBias_float)\n",
        "print(float_to_fixed_scale(act_scalesDict, weight_scalesDict, outputBias_float))\n",
        "with open('fixed_scale.json', 'w', newline='') as jsonfile:\n",
        "    json.dump(scalesDict, jsonfile)\n",
        "zf.write('./fixed_scale.json')\n",
        "\n",
        "\n",
        "np.savetxt('./weights/output.fc.bias.csv', outputBias, delimiter=',')\n",
        "zf.write('./weights/output.fc.bias.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxIDG7gtjCBB"
      },
      "source": [
        "### Reconstruct the model with build-in function and extract the input and output of layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AllumB6-YI9B"
      },
      "outputs": [],
      "source": [
        "def getWeightAndScale():\n",
        "    weightsDict = {}\n",
        "    shapeDict = {\"conv1.conv\": [6, 1, 5, 5],\n",
        "                 \"conv3.conv\": [16, 6, 5, 5],\n",
        "                 \"conv5.conv\": [120, 16, 5, 5],\n",
        "                 \"fc6.fc\": [84, 120],\n",
        "                 \"output.fc\": [10, 84]\n",
        "                 }\n",
        "\n",
        "    for key in shapeDict:\n",
        "        Arr = np.loadtxt('./weights/'+key+\".weight.csv\",\n",
        "                         delimiter=',').astype(int)\n",
        "        shape = shapeDict[key]\n",
        "        Arr = Arr.reshape(([i for i in shape]))\n",
        "        weightsDict[key] = Arr\n",
        "\n",
        "    weightsDict[\"outputBias\"] = np.loadtxt(\n",
        "        './weights/'+key+\".bias.csv\", delimiter=',').reshape(([1, 10])).astype(float)\n",
        "\n",
        "    scalesDict = {}\n",
        "    with open('fixed_scale.json') as json_file:\n",
        "        scalesDict = json.load(json_file)\n",
        "    for i in scalesDict:\n",
        "      scalesDict[i] = np.array([scalesDict[i]])\n",
        "\n",
        "    return weightsDict, scalesDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWPqoj8tf0Rr"
      },
      "outputs": [],
      "source": [
        "class QAT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QAT, self).__init__()\n",
        "        self.weightsDict, self.scalesDict = getWeightAndScale()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5, bias=False)\n",
        "        self.conv1.weight.data = torch.from_numpy(self.weightsDict[\"conv1.conv\"]).float()\n",
        "\n",
        "        self.maxpool2 = nn.Sequential(OrderedDict([\n",
        "            ('pool', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "\n",
        "        self.conv3 = nn.Conv2d(6, 16, 5, bias=False)\n",
        "        self.conv3.weight.data = torch.from_numpy(self.weightsDict[\"conv3.conv\"]).float()\n",
        "\n",
        "        self.maxpool4 = nn.Sequential(OrderedDict([\n",
        "            ('pool', nn.MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "        ]))\n",
        "\n",
        "        self.conv5 = nn.Conv2d(16, 120, 5, bias=False)\n",
        "        self.conv5.weight.data = torch.from_numpy(self.weightsDict[\"conv5.conv\"]).float()\n",
        "\n",
        "\n",
        "        self.fc6 = nn.Linear(120, 84, bias=False)\n",
        "        self.fc6.weight.data = torch.from_numpy(self.weightsDict[\"fc6.fc\"]).float()\n",
        "\n",
        "        self.output = nn.Linear(84, 10, bias=True)\n",
        "        self.output.weight.data = torch.from_numpy(self.weightsDict[\"output.fc\"]).float()\n",
        "        self.output.bias.data = torch.from_numpy(self.weightsDict[\"outputBias\"].reshape(1, 10)).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.clamp(torch.quantize_per_tensor(x, torch.from_numpy(1/self.scalesDict[\"quant\"]),\n",
        "                  torch.tensor(0), torch.qint32).int_repr(), -128, 127).float()\n",
        "        x = self.conv1(x)\n",
        "        x = torch.clamp(torch.quantize_per_tensor(x, torch.from_numpy(1/self.scalesDict[\"conv1.conv\"]),\n",
        "                  torch.tensor(0), torch.qint32).int_repr() >> 16, 0, 127).float()\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.clamp(torch.quantize_per_tensor(x, torch.from_numpy(1/self.scalesDict[\"conv3.conv\"]),\n",
        "                  torch.tensor(0), torch.qint32).int_repr() >> 16, 0, 127).float()\n",
        "\n",
        "        x = self.maxpool4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = torch.clamp(torch.quantize_per_tensor(x, torch.from_numpy(1/self.scalesDict[\"conv5.conv\"]),\n",
        "                  torch.tensor(0), torch.qint32).int_repr() >> 16, 0, 127).float()\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc6(x)\n",
        "        x = torch.clamp(torch.quantize_per_tensor(x, torch.from_numpy(1/self.scalesDict[\"fc6.fc\"]),\n",
        "                  torch.tensor(0), torch.qint32).int_repr() >> 16, 0, 127).float()\n",
        "\n",
        "        x = self.output(x)\n",
        "        x = torch.clamp(torch.quantize_per_tensor(x, torch.from_numpy(1/self.scalesDict[\"output.fc\"]),\n",
        "                  torch.tensor(0), torch.qint32).int_repr() >> 16, -128, 127)\n",
        "\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNqhZl86jCBC",
        "outputId": "94366654-d385-4eda-bf1c-277a5dd0cea0"
      },
      "outputs": [],
      "source": [
        "qat = QAT()\n",
        "score = test(qat, testloader)\n",
        "print('Accuracy of the network with fixed point scale: {}%'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TfpcnC60_g4"
      },
      "source": [
        "Use an image as an input of the activations，and choose 100 images to generate patterns for our homework 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CByZgw-ftMBD"
      },
      "outputs": [],
      "source": [
        "# random choose images as the input and get the output\n",
        "np.random.seed(0)\n",
        "index = np.random.randint(0,len(trainset), size=100)\n",
        "index = range(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9M4QXt60WmL"
      },
      "source": [
        "Save the activations of input and output to the CSV format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSIdYFyZshFj"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./activations'):\n",
        "    os.mkdir('./activations')\n",
        "for ind in range(100):\n",
        "    if not os.path.exists('./activations/img{}'.format(ind)):\n",
        "        os.mkdir('./activations/img{}'.format(ind))\n",
        "\n",
        "    for name, model in qat.named_children():\n",
        "        model.profile_activations = True\n",
        "        model.register_forward_hook(visualize_activations)\n",
        "    input0, label = testset[index[ind]]\n",
        "    input = input0.reshape(1, 1, 32, 32)\n",
        "    output = qat(input)\n",
        "    for name, model in qat.named_children(): model.profile_activations = False\n",
        "\n",
        "\n",
        "    np.savetxt('./activations/img{}/input.csv'.format(ind), input.cpu().data.numpy().reshape(-1), delimiter=',')\n",
        "    np.savetxt('./activations/img{}/output.csv'.format(ind), output.cpu().data.numpy().reshape(-1).astype(int), delimiter=',')\n",
        "    zf.write('./activations/img{}/input.csv'.format(ind))\n",
        "    zf.write('./activations/img{}/output.csv'.format(ind))\n",
        "\n",
        "    opDict = {\n",
        "        'conv1': (qat.conv1.inAct, qat.conv1.outAct),\n",
        "        'maxpool2': (qat.maxpool2.inAct, qat.maxpool2.outAct),\n",
        "        'conv3': (qat.conv3.inAct, qat.conv3.outAct),\n",
        "        'maxpool4': (qat.maxpool4.inAct, qat.maxpool4.outAct),\n",
        "        'conv5': (qat.conv5.inAct, qat.conv5.outAct),\n",
        "        'fc6': (qat.fc6.inAct, qat.fc6.outAct),\n",
        "        'output': (qat.output.inAct, qat.output.outAct)\n",
        "    }\n",
        "\n",
        "    for key in opDict:\n",
        "        if not os.path.exists('./activations/img{}/{}'.format(ind, key)):\n",
        "            os.mkdir('./activations/img{}/{}'.format(ind, key))\n",
        "        if(opDict[key][0].type()== \"torch.quantized.QInt8Tensor\" or opDict[key][0].type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "            temp = opDict[key][0].cpu().int_repr()\n",
        "        else:\n",
        "            temp = opDict[key][0].cpu()\n",
        "        if(opDict[key][1].type()== \"torch.quantized.QInt8Tensor\" or opDict[key][1].type()== \"torch.quantized.QUInt8Tensor\"):\n",
        "            temp1 = opDict[key][1].cpu().int_repr()\n",
        "        else:\n",
        "            temp1 = opDict[key][1].cpu()\n",
        "        np.savetxt('./activations/img{}/{}/input.csv'.format(ind, key), temp.data.numpy().reshape(-1).astype(float), delimiter=',')\n",
        "        np.savetxt('./activations/img{}/{}/output.csv'.format(ind, key), temp1.cpu().data.numpy().reshape(-1).astype(float), delimiter=',')\n",
        "        zf.write('./activations/img{}/{}/input.csv'.format(ind, key))\n",
        "        zf.write('./activations/img{}/{}/output.csv'.format(ind, key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbUYA1VpKLAB"
      },
      "source": [
        "Save the zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlOLRLdTJqcm"
      },
      "outputs": [],
      "source": [
        "zf.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aa3a4132232ef57bd967da4f502201fad326f014cc6ad01b5db327eca3c579df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
